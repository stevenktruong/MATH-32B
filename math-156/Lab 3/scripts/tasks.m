% Overview% ----------% plotSample              to plot data% plotBoundaries          used to plot boundaries with 3 classes quickly% plotDecisionBoundaries  plot the decision boundary between two classes% accuracy                to see the accuracy of the linear classifier for a sample D% knnAccuracy             accuracy of k-nearest neighbors classification% D  contains the trained data% Y  contains the testing data% W  contains the linear coefficient%% Should work for arbitrary number of classes, except for the k-nearest neighbors,% it will always display 3 correct classifications. Boundaries have to be drawn% manually, though.%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Parameters%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%global SAMPLE_SIZE; % Number of samples to generate from each distributionglobal CLASSES;     % Number of classesSAMPLE_SIZE = 50;CLASSES = 3;% Means% The i-th row is the mean of the i-th data setglobal mu;mu(1, :) = [2 2];mu(2, :) = [2 4];mu(3, :) = [3 3];% Covariances% The i-th page is the covariance of the i-th data setglobal sigma;sigma(:, :, 1) = [0.2 0.05; 0.05 0.3];sigma(:, :, 2) = [0.4 -0.1; -0.1 0.3];sigma(:, :, 3) = [0.5 -0.3; -0.3 0.4];%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Helper Function%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Function to generate data from mu and sigma% The i-th page of D is D_i, whose observations are row vectorsfunction Y = generateSample  global SAMPLE_SIZE CLASSES mu sigma;  Y = zeros(SAMPLE_SIZE, 2, CLASSES);  for i=1:CLASSES    Y(:, :, i) = mvnrnd(mu(i, :), sigma(:, :, i), SAMPLE_SIZE);  endend% D is a SAMPLE_SIZE x 2 x CLASSES matrix% I.e., each page is the sample with each observation a row vectorfunction plotSample(D)  global CLASSES;  colors = ['b' 'r' 'g']; % Color for each class  for i=1:CLASSES    plot(D(:, 1, i), D(:, 2, i), strcat(colors(i), '*'));  endend% Calculate the least squares coefficientfunction W = findCoefficient(D)  global SAMPLE_SIZE CLASSES;  % i-th class column vector is e(:, i)  % Also doubles as the canonical basis vectors  e = eye(CLASSES);    % Calculate XX^T and TX^T (observations are row vectors in each page of D)  XXT = zeros(3, 3);  TXT = zeros(CLASSES, 3);  for k=1:SAMPLE_SIZE    for l=1:CLASSES      % Vectors are of the form (x, 1) to absorb the bias      XXT = XXT + [D(k, :, l) 1]' * [D(k, :, l) 1];      TXT = TXT + e(:, l) * [D(k, :, l) 1];    end   end  % WXX^T = TX^T <=> XX^T * W^T = (TX^T)'  W = linsolve(XXT, TXT')';end% Plot the decision boundary between classes i and jfunction plotDecisionBoundary(i, j, color)  global SAMPLE_SIZE CLASSES D W;  e = eye(CLASSES);    % Calculate decision boundary basis for classes i and j  % ei^TWx = ej^TWx <=> (ei - ej)^TWx = 0  % The third column of W is the bias term, so we don't want it here  boundary_basis = null((e(i, :) - e(j, :)) * W(:, 1:2));    % Calculate the mean of the data  xbar = mean(mean(D, 1), 3)';    % Decision boundary  scales = linspace(-1.5, 1.5);  points = (scales .* boundary_basis)' .+ xbar';  plot(points(:, 1), points(:, 2), color);end% Plot the boundariesfunction plotBoundaries  plotDecisionBoundary(1, 2, 'm');  plotDecisionBoundary(3, 2, 'k');  plotDecisionBoundary(3, 1, 'c');end% Calculate accuracy of the decision boundariesfunction accuracy = accuracy(D)  global SAMPLE_SIZE CLASSES W;  accurate = 0;  for i=1:CLASSES    for j=1:SAMPLE_SIZE      [value, index] = max(W * [D(j, :, i) 1]');      if (index == i)        accurate = accurate + 1;      end    end  end  accuracy = accurate / (CLASSES * SAMPLE_SIZE);end% D is the trained data, Y is untrained, k is number of nearest neighbors% correct(i) is how often class i was classified correctlyfunction [accuracy, correct] = knnAccuracy(D, Y, k)  global SAMPLE_SIZE CLASSES;  % Reshape the data and put everything in one large matrix  %   Row   1  -  50  Class 1  %   Row  51  - 100  Class 2  %   Row 101  - 150  Class 3  % fastKNN needs the last column to be the class label of each row  % for our trained data. We do it for the untrained data so we can  % easily check the accuracy.  Dn = num2cell(D, [1, 2]);  Yn = num2cell(Y, [1, 2]);  Dn = vertcat(Dn{:});  Yn = vertcat(Yn{:});  for i=1:CLASSES    for j=1:SAMPLE_SIZE      row = (i - 1) * SAMPLE_SIZE + j;      Dn(row, 3) = i; % Assign the third column to be the class      Yn(row, 3) = i;    end  end   accurate = 0;  correctList = [];  for i=1:(SAMPLE_SIZE*CLASSES)    c = fastKNN(Dn, Yn(i, 1:2), k);    if (c == Yn(i, 3))      correctList = [correctList Yn(i, 3)]; % Add the class number to the end      accurate = accurate + 1;    endif  end  accuracy = accurate / (SAMPLE_SIZE * CLASSES);  for i=1:CLASSES    correct(i) = sum(correctList(:) == i);  endend%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Main Script%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%clc;% Data Samples% D is data to use to generate the decision boundaries% Y is second set of data to gauge accuracyglobal D;D = generateSample;Y = generateSample;% Linear Coefficient% W is global to make sure that the above functions use the exact same% coefficient when calculating thingsglobal W;W = findCoefficient(D);% Draw scatter plots with decision boundariesclose all;hold on;plotSample(D);plotBoundaries;legend('D1', 'D2', 'D3', 'C1,2', 'C2,3', 'C1,3');titleD = strcat('Training Data, Accuracy:', {' '}, num2str(accuracy(D)));title(titleD, 'FontSize', 16);figure;hold on;plotSample(Y);plotBoundaries;legend('Y1', 'Y2', 'Y3', 'C1,2', 'C2,3', 'C1,3');titleY = strcat('Testing Data, Accuracy:', {' '}, num2str(accuracy(Y)));title(titleY, 'FontSize', 16);% k-nearest neighbors accuracyfprintf('K-Nearest Neighbors Classification\n\n');k = 15; % Number of values of k to tryaccuracyList = zeros(k, CLASSES+1);for i=1:k  [accuracy, correctList] = knnAccuracy(D, Y, i);  accuracyList(i, :) = [accuracy, correctList];end[accuracy, best] = max(accuracyList(:, 1));% Print in console% 1, 2, and 3 are the number of times that class was classified correctlyfprintf('       k  Accuracy   1   2   3\n');for i=1:k  fprintf('      %2d  %f  %2d  %2d  %2d\n', i, accuracyList(i, 1:4));endfprintf('\nBest: %2d  %f  %2d  %2d  %2d\n', best, accuracyList(best, 1:4));% Plot bar graph of correct classificationsfigure;bar(accuracyList(best, 2:4));title('Correct Classifications', 'FontSize', 16);